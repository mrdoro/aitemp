{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# งานที่ 4: อินเทอร์เฟซการสนทนา - แชทกับ Llama 3 และ Titan Premier LLM\n",
    "\n",
    "ในโน้ตบุ๊กนี้ คุณจะสร้างแชทบอทโดยใช้ llama3-8b-instruct และ titan-text-premier Foundation Models (FM) ใน Amazon Bedrock\n",
    "\n",
    "อินเทอร์เฟซการสนทนา เช่น แชทบอทและผู้ช่วยเสมือนสามารถเพิ่มประสบการณ์การใช้งานให้กับลูกค้าของคุณได้ แชทบอทจะใช้การประมวลผลภาษาธรรมชาติ (NLP) และอัลกอริทึมแมชชีนเลิร์นนิง เพื่อทำความเข้าใจและตอบกลับคำถามของผู้ใช้ คุณสามารถใช้แชทบอทในแอปพลิเคชันที่หลากหลาย เช่น การบริการลูกค้า การขาย และอีคอมเมิร์ซ เพื่อให้การตอบกลับผู้ใช้นั้นรวดเร็วและมีประสิทธิภาพ ผู้ใช้สามารถเข้าถึงได้ผ่านช่องทางต่างๆ เช่น เว็บไซต์ แพลตฟอร์มโซเชียลมีเดีย และแอปส่งข้อความ\n",
    "\n",
    "- **แชทบอท (พื้นฐาน)**, แชทบอท Zero Shot กับโมเดล FM\n",
    "- **แชทบอทโดยใช้พรอมต์คำสั่ง**, เทมเพลต (LangChain) - แชทบอทที่มีบริบทบางอย่างที่ให้ไว้ในเทมเพลตพรอมต์คำสั่ง\n",
    "- **แชทบอทกับบุคคล**, แชทบอทที่มีบทบาทตามที่กำหนด เช่น โค้ชอาชีพ และการปฏิสัมพันธ์กับมนุษย์\n",
    "- **แชทบอทที่ตระหนักถึงบริบท** ส่งผ่านไฟล์ภายนอกโดยสร้างการฝังข้อมูล\n",
    "\n",
    "## เฟรมเวิร์ก LangChain สำหรับการสร้าง Chatbot กับ Amazon Bedrock\n",
    "\n",
    "ในอินเทอร์เฟซการสนทนา เช่น แชทบอท การจดจำปฏิสัมพันธ์ก่อนหน้านี้มีความสำคัญอย่างยิ่งทั้งในระดับระยะสั้นและระยะยาว\n",
    "\n",
    "เฟรมเวิร์ก LangChain มีส่วนประกอบหน่วยความจำในสองรูปแบบ รูปแบบแรกคือ LangChain มียูทิลิตี้ผู้ช่วยสำหรับการจัดการและจัดการข้อความแชทก่อนหน้า สิ่งเหล่านี้ได้รับการออกแบบให้เป็นแบบแยกส่วน รูปแบบที่สองคือ LangChain มีวิธีง่ายๆ ในการรวมยูทิลิตี้เหล่านี้เข้ากับห่วงโซ่ ช่วยให้คุณสามารถกำหนดและโต้ตอบกับนามธรรมประเภทต่างๆ ได้อย่างง่ายดาย ซึ่งทำให้คุณสร้างแชทบอทที่มีประสิทธิภาพได้อย่างง่ายดาย\n",
    "\n",
    "## การสร้างแชทบอทด้วยบริบท - องค์ประกอบสำคัญ\n",
    "\n",
    "กระบวนการแรกในการสร้างแชทบอทที่ตระหนักถึงบริบทคือการสร้างการฝังสำหรับบริบท โดยปกติ คุณมีกระบวนการให้ข้อมูลที่ทำงานผ่านโมเดลการฝังของคุณและสร้างการฝังซึ่งจะถูกเก็บไว้ในเวกเตอร์สโตร์ ในโน้ตบุ๊กนี้ ให้คุณใช้รุ่น Titan Embeddings สำหรับสิ่งนี้ กระบวนการที่สองคือการจัดการคำขอของผู้ใช้ การโต้ตอบ การเรียกใช้ และการส่งคืนเอาต์พุต ซึ่งเกี่ยวข้องกับการจัดระเบียบคำขอของผู้ใช้ การโต้ตอบกับโมเดล/ส่วนประกอบที่จำเป็นเพื่อรวบรวมข้อมูล การเรียกแชทบอทเพื่อกำหนดการตอบกลับ จากนั้นก็เป็นการส่งการตอบกลับจากแชทบอทไปยังผู้ใช้"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## งานที่ 4.1: การตั้งค่าสภาพแวดล้อม\n",
    "\n",
    "ในงานนี้ ให้ตั้งค่าสภาพแวดล้อมของคุณ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#ignore warnings and create a service client by name using the default session.\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "import boto3\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "module_path = \"..\"\n",
    "sys.path.append(os.path.abspath(module_path))\n",
    "bedrock_client = boto3.client('bedrock-runtime',region_name=os.environ.get(\"AWS_DEFAULT_REGION\", None))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# format instructions into a conversational prompt\n",
    "from typing import Dict, List\n",
    "\n",
    "def format_instructions(instructions: List[Dict[str, str]]) -> List[str]:\n",
    "    \"\"\"Format instructions where conversation roles must alternate system/user/assistant/user/assistant/...\"\"\"\n",
    "    prompt: List[str] = []\n",
    "    for instruction in instructions:\n",
    "        if instruction[\"role\"] == \"system\":\n",
    "            prompt.extend([\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\", (instruction[\"content\"]).strip(), \" <|eot_id|>\"])\n",
    "        elif instruction[\"role\"] == \"user\":\n",
    "            prompt.extend([\"<|start_header_id|>user<|end_header_id|>\\n\", (instruction[\"content\"]).strip(), \" <|eot_id|>\"])\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid role: {instruction['role']}. Role must be either 'user' or 'system'.\")\n",
    "    prompt.extend([\"<|start_header_id|>assistant<|end_header_id|>\\n\"])\n",
    "    return \"\".join(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## งานที่ 4.2: การใช้ประวัติการแชทจาก LangChain เพื่อเริ่มการสนทนา\n",
    "\n",
    "ในงานนี้ ให้คุณเปิดใช้งานแชทบอทเพื่อนำบริบทการสนทนาผ่านการโต้ตอบกับผู้ใช้หลายครั้ง การมีหน่วยความจำในการสนทนาเป็นสิ่งสำคัญสำหรับแชทบอทในการจัดบทสนทนาที่มีความหมายและสอดคล้องกันเมื่อเวลาผ่านไป\n",
    "\n",
    "คุณใช้ความสามารถของหน่วยความจำการสนทนาโดยสร้างบนชั้นเรียน InMemoryChatMessageHistory ของ LangChain อ็อบเจ็กต์นี้จะเก็บการสนทนาระหว่างผู้ใช้และแชทบอท โดยประวัติสามารถใช้งานเอเจนต์แชทบอทเพื่อให้สามารถใช้ประโยชน์จากบริบทจากการสนทนาก่อนหน้านี้ได้\n",
    "\n",
    "<i aria-hidden=\"true\" class=\"fas fa-sticky-note\" style=\"color:#563377\"></i> **หมายเหตุ:** เอาต์พุตของโมเดลไม่สามารถแก้ไขได้"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_aws import ChatBedrock\n",
    "\n",
    "chat_model=ChatBedrock(\n",
    "    model_id=\"meta.llama3-8b-instruct-v1:0\" , \n",
    "    client=bedrock_client)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"Answer the following questions as best you can.\"),\n",
    "        (\"placeholder\", \"{chat_history}\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "history = InMemoryChatMessageHistory()\n",
    "\n",
    "\n",
    "def get_history():\n",
    "    return history\n",
    "\n",
    "\n",
    "chain = prompt | chat_model | StrOutputParser()\n",
    "\n",
    "wrapped_chain = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_history,\n",
    "    history_messages_key=\"chat_history\",\n",
    ")\n",
    "query=\"how are you?\"\n",
    "response=wrapped_chain.invoke({\"input\": query})\n",
    "# Printing history to see the history being built out. \n",
    "print(history)\n",
    "# For the rest of the conversation, the output will only include response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### คำถามใหม่\n",
    "\n",
    "โมเดลตอบกลับด้วยข้อความเริ่มต้น ตอนนี้ ให้คุณถามคำถามสองสามข้อ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#new questions\n",
    "instructions = [{\"role\": \"user\", \"content\": \"Give me a few tips on how to start a new garden.\"}]\n",
    "response=wrapped_chain.invoke({\"input\": format_instructions(instructions)})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### สร้างจากคำถาม\n",
    "\n",
    "ตอนนี้ ให้ถามคำถามโดยไม่ต้องพูดถึงคำว่า “สวน” เพื่อดูว่าโมเดลสามารถเข้าใจการสนทนาก่อนหน้านี้ได้หรือไม่"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# build on the questions\n",
    "instructions = [{\"role\": \"user\", \"content\": \"bugs\"}]\n",
    "response=wrapped_chain.invoke({\"input\": format_instructions(instructions)})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### เสร็จสิ้นการสนทนานี้"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# finishing the conversation\n",
    "instructions = [{\"role\": \"user\", \"content\": \"That's all, thank you!\"}]\n",
    "response=wrapped_chain.invoke({\"input\": format_instructions(instructions)})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## งานที่ 4.3: แชทบอทโดยใช้เทมเพลตพรอมต์คำสั่ง (LangChain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ในงานนี้ ให้คุณใช้ PromptTemplate เริ่มต้นที่รับผิดชอบในการสร้างอินพุตนี้ LangChain มีคลาสและฟังก์ชั่นหลายอย่างเพื่อให้การสร้างและการทำงานกับพรอมต์คำสั่งทำได้ง่าย"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  prompt for a conversational agent\n",
    "def format_prompt(actor:str, input:str):\n",
    "    formatted_prompt: List[str] = []\n",
    "    if actor == \"system\":\n",
    "        prompt_template=\"\"\"<|begin_of_text|><|start_header_id|>{actor}<|end_header_id|>\\n{input}<|eot_id|>\"\"\"\n",
    "    elif actor == \"user\":\n",
    "        prompt_template=\"\"\"<|start_header_id|>{actor}<|end_header_id|>\\n{input}<|eot_id|>\"\"\"\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid role: {actor}. Role must be either 'user' or 'system'.\")   \n",
    "    prompt = PromptTemplate.from_template(prompt_template)     \n",
    "    formatted_prompt.extend(prompt.format(actor=actor,input=input))\n",
    "    formatted_prompt.extend([\"<|start_header_id|>assistant<|end_header_id|>\\n\"])\n",
    "    return \"\".join(formatted_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# chat user experience\n",
    "import ipywidgets as ipw\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "class ChatUX:\n",
    "    \"\"\" A chat UX using IPWidgets\n",
    "    \"\"\"\n",
    "    def __init__(self, qa, retrievalChain = False):\n",
    "        self.qa = qa\n",
    "        self.name = None\n",
    "        self.b=None\n",
    "        self.retrievalChain = retrievalChain\n",
    "        self.out = ipw.Output()\n",
    "\n",
    "\n",
    "    def start_chat(self):\n",
    "        print(\"Starting chat bot\")\n",
    "        display(self.out)\n",
    "        self.chat(None)\n",
    "\n",
    "\n",
    "    def chat(self, _):\n",
    "        if self.name is None:\n",
    "            prompt = \"\"\n",
    "        else: \n",
    "            prompt = self.name.value\n",
    "        if 'q' == prompt or 'quit' == prompt or 'Q' == prompt:\n",
    "            with self.out:\n",
    "                print(\"Thank you , that was a nice chat !!\")\n",
    "            return\n",
    "        elif len(prompt) > 0:\n",
    "            with self.out:\n",
    "                thinking = ipw.Label(value=\"Thinking...\")\n",
    "                display(thinking)\n",
    "                try:\n",
    "                    if self.retrievalChain:\n",
    "                        response = self.qa.invoke({\"input\": prompt})\n",
    "                        result=response['answer']\n",
    "                    else:\n",
    "                        instructions = [{\"role\": \"user\", \"content\": prompt}]\n",
    "                        #result = self.qa.invoke({'input': format_prompt(\"user\",prompt)}) #, 'history':chat_history})\n",
    "                        result = self.qa.invoke({\"input\": format_instructions(instructions)})\n",
    "                except:\n",
    "                    result = \"No answer\"\n",
    "                thinking.value=\"\"\n",
    "                print(f\"AI:{result}\")\n",
    "                self.name.disabled = True\n",
    "                self.b.disabled = True\n",
    "                self.name = None\n",
    "\n",
    "        if self.name is None:\n",
    "            with self.out:\n",
    "                self.name = ipw.Text(description=\"You:\", placeholder='q to quit')\n",
    "                self.b = ipw.Button(description=\"Send\")\n",
    "                self.b.on_click(self.chat)\n",
    "                display(ipw.Box(children=(self.name, self.b)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ถัดไป ให้เริ่มแชท"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# start chat\n",
    "history = InMemoryChatMessageHistory() #reset chat history\n",
    "chat = ChatUX(wrapped_chain)\n",
    "chat.start_chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## งานที่ 4.4: แชทบอทที่มีบุคลิกภาพ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ในงานนี้ ผู้ช่วยปัญญาประดิษฐ์ (AI) จะมีบทบาทเป็นโค้ชอาชีพ คุณสามารถแจ้งแชทบอทเกี่ยวกับบุคลิกภาพ (หรือบทบาท) โดยใช้ข้อความระบบ ใช้ประโยชน์จากการสอนเรื่อง InMemoryChatMessageHistory ต่อไป เพื่อรักษาบริบทการสนทนา"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \" You will be acting as a career coach. Your goal is to give career advice to users. For questions that are not career related, don't provide advice. Say, I don't know.\"),\n",
    "        (\"placeholder\", \"{chat_history}\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "history = InMemoryChatMessageHistory() # reset history\n",
    "\n",
    "chain = prompt | chat_model | StrOutputParser()\n",
    "\n",
    "wrapped_chain = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_history,\n",
    "    history_messages_key=\"career_chat_history\",\n",
    ")\n",
    "\n",
    "response=wrapped_chain.invoke({\"input\": \"What are the career options in AI?\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response=wrapped_chain.invoke({\"input\": \"How to fix my car?\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ตอนนี้ ให้ถามคำถามที่ไม่ได้อยู่ในความพิเศษของบุคคลนี้ โมเดลไม่ควรตอบคำถามนั้นและควรให้เหตุผลสำหรับสิ่งนั้น"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## งาน 4.5 แชทบอทพร้อมบริบท\n",
    "\n",
    "ในงานนี้ ให้คุณขอให้แชทบอทตอบคำถามตามบริบทที่ส่งต่อไป คุณใช้ไฟล์ CSV และใช้โมเดล Titan Embeddings เพื่อสร้างเวกเตอร์ที่เป็นตัวแทนบริบทนั้น เวกเตอร์นี้ถูกเก็บไว้ใน Facebook AI Similarity Search (FAISS) เมื่อแชทบอทถูกถามคำถาม คุณจะส่งเวกเตอร์นี้กลับไปยังแชทบอทและขอให้ดึงคำตอบโดยใช้เวกเตอร์"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### โมเดลการฝัง Titan\n",
    "\n",
    "การฝังจะแสดงคำ วลี หรือรายการที่ไม่ต่อเนื่องอื่นๆ เป็นเวกเตอร์ในพื้นที่เวกเตอร์ต่อเนื่อง สิ่งนี้ช่วยให้โมเดลแมชชีนเลิร์นนิงสามารถดำเนินการทางคณิตศาสตร์ในการแสดงเหล่านี้และจับภาพความสัมพันธ์เชิงความหมายเหล่านี้\n",
    "\n",
    "คุณใช้การฝังสำหรับการสร้างการดึงข้อมูลเพิ่มเติม (RAG) [ความสามารถในการค้นหาเอกสาร] (https://labelbox.com/blog/how-vector-similarity-search-works/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model configuration\n",
    "from langchain_aws.embeddings import BedrockEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "br_embeddings = BedrockEmbeddings(model_id=\"amazon.titan-embed-text-v1\", client=bedrock_client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FAISS เป็น VectorStore\n",
    "\n",
    "ในการใช้การฝังสำหรับการค้นหา คุณต้องมีร้านค้าที่สามารถทำการค้นหาความคล้ายคลึงกันของเวกเตอร์ได้อย่างมีประสิทธิภาพ ในโน้ตบุ๊กนี้ ให้คุณใช้ FAISS ซึ่งเป็นที่เก็บในหน่วยความจำ ในการจัดเก็บเวกเตอร์อย่างถาวร คุณสามารถใช้ฐานความรู้สำหรับ Amazon Bedrock, pgVector, Pinecone, Weaviate หรือ Chroma ได้\n",
    "\n",
    "API ของ LangChain VectorStore อยู่ [ที่นี่](https://python.langchain.com/v0.2/docs/integrations/vectorstores/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# vector store\n",
    "from langchain.document_loaders import CSVLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.indexes.vectorstore import VectorStoreIndexWrapper\n",
    "\n",
    "loader = CSVLoader(\"../rag_data/Amazon_SageMaker_FAQs.csv\") # --- > 219 docs with 400 chars\n",
    "documents_aws = loader.load() #\n",
    "print(f\"documents:loaded:size={len(documents_aws)}\")\n",
    "\n",
    "docs = CharacterTextSplitter(chunk_size=2000, chunk_overlap=400, separator=\",\").split_documents(documents_aws)\n",
    "\n",
    "print(f\"Documents:after split and chunking size={len(docs)}\")\n",
    "vectorstore_faiss_aws = None\n",
    "try:\n",
    "    \n",
    "    vectorstore_faiss_aws = FAISS.from_documents(\n",
    "        documents=docs,\n",
    "        embedding = br_embeddings, \n",
    "        #**k_args\n",
    "    )\n",
    "\n",
    "    print(f\"vectorstore_faiss_aws:created={vectorstore_faiss_aws}::\")\n",
    "\n",
    "except ValueError as error:\n",
    "    if  \"AccessDeniedException\" in str(error):\n",
    "        print(f\"\\x1b[41m{error}\\\n",
    "        \\nTo troubeshoot this issue please refer to the following resources.\\\n",
    "         \\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/troubleshoot_access-denied.html\\\n",
    "         \\nhttps://docs.aws.amazon.com/bedrock/latest/userguide/security-iam.html\\x1b[0m\\n\")      \n",
    "        class StopExecution(ValueError):\n",
    "            def _render_traceback_(self):\n",
    "                pass\n",
    "        raise StopExecution        \n",
    "    else:\n",
    "        raise error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### เรียกใช้การทดสอบรหัสต่ำอย่างรวดเร็ว \n",
    "\n",
    "คุณสามารถใช้คลาส Wrapper ที่จัดทำโดย LangChain เพื่อสืบค้นที่เก็บฐานข้อมูลเวกเตอร์และส่งคืนเอกสารที่เกี่ยวข้อง สิ่งนี้จะเรียกใช้ QA Chain ที่มีค่าเริ่มต้นทั้งหมด"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chat_llm=ChatBedrock(\n",
    "    model_id=\"amazon.titan-text-premier-v1:0\" , \n",
    "    client=bedrock_client)\n",
    "# wrapper store faiss\n",
    "wrapper_store_faiss = VectorStoreIndexWrapper(vectorstore=vectorstore_faiss_aws)\n",
    "print(wrapper_store_faiss.query(\"R in SageMaker\", llm=chat_llm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### แอปพลิเคชันแชทบอท\n",
    "\n",
    "สำหรับแชทบอท คุณต้องมีการจัดการบริบท ประวัติ ที่จัดเก็บเวกเตอร์ และส่วนประกอบอื่นๆ อีกมากมาย คุณเริ่มต้นด้วยการสร้างห่วงโซ่ Recrieval Augmented Generation (RAG) ที่รองรับบริบท\n",
    "\n",
    "สิ่งนี้ใช้ฟังก์ชัน **create_stuff_documents_chain** และ**create_retrieval_chain**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### พารามิเตอร์และฟังก์ชั่นที่ใช้สำหรับ RAG\n",
    "\n",
    "- **Retriever:** คุณใช้ `VectorStoreRetriever` ซึ่งได้รับการสนับสนุนโดย `VectorStore` ในการดึงข้อความ มีสองประเภทการค้นหาให้เลือก คือ `\"similarity\"` หรือ `\"mmr\"` โดย `search_type=\"similarity\"` จะใช้การค้นหาความคล้ายคลึงกันในอ็อบเจ็กต์รีทริเวอร์ ซึ่งจะเลือกเวกเตอร์ชิ้นข้อความที่คล้ายกับเวกเตอร์คำถามมากที่สุด\n",
    "\n",
    "- **create_stuff_documents_chain** จะระบุว่าบริบทที่ดึงข้อมูลจะถูกป้อนเข้าสู่พรอมต์คำสั่งและ LLM อย่างไร เอกสารที่ดึงมาจะเป็น “stuffed” ตามบริบทโดยไม่มีการสรุปหรือการประมวลผลอื่นๆ ลงในพรอมต์คำสั่ง\n",
    "\n",
    "- **create_retrieval_chain** จะเพิ่มขั้นตอนการดึงข้อมูลและเผยแพร่บริบทที่ดึงข้อมูลผ่านห่วงโซ่โดยให้พร้อมกับคำตอบสุดท้าย \n",
    "\n",
    "หากคำถามที่ถามอยู่นอกขอบเขตของบริบทโมเดลจะตอบว่าไม่ทราบคำตอบ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "retriever=vectorstore_faiss_aws.as_retriever()\n",
    "question_answer_chain = create_stuff_documents_chain(chat_llm, prompt)\n",
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)\n",
    "\n",
    "response = rag_chain.invoke({\"input\": \"What is sagemaker?\"})\n",
    "print(response) # shows the document chunks consulted to come up with the answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ถัดไป ให้เริ่มแชท"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = ChatUX(rag_chain, retrievalChain=True)\n",
    "chat.start_chat()  # Only answers will be shown here, and not the citations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "คุณได้ใช้ Titan LLM เพื่อสร้างอินเทอร์เฟซการสนทนาด้วยรูปแบบต่อไปนี้:\n",
    "\n",
    "- แชทบอท (พื้นฐาน - ไม่มีบริบท)\n",
    "- แชทบอทโดยใช้เทมเพลตพรอมต์คำสั่ง (Langchain)\n",
    "- แชทบอทกับบุคคล\n",
    "- แชทบอทที่มีบริบท"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ลองด้วยตัวเอง\n",
    "\n",
    "- เปลี่ยนพรอมต์คำสั่งตามฐานการใช้งานเฉพาะของคุณและประเมินเอาต์พุตของโมเดลต่างๆ\n",
    "- เล่นด้วยความยาวโทเค็นเพื่อทำความเข้าใจเวลาแฝงและการตอบกลับของบริการ\n",
    "- ใช้หลักการวิศวกรรมการโต้ตอบที่แตกต่างกันเพื่อให้ได้เอาต์พุตที่ดีขึ้น\n",
    "\n",
    "### เก็บงาน\n",
    "\n",
    "คุณทำโน้ตบุ๊กนี้เสร็จแล้ว หากต้องการย้ายไปยังส่วนถัดไปของแล็บ ให้ทำดังนี้\n",
    "\n",
    "- ปิดไฟล์โน้ตบุ๊กนี้และไปต่อยัง **งานที่ 5**"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}